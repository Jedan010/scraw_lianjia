{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 抓取链家小区数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 思路 ** ：\n",
    "1. 获取一个小区页面的数据\n",
    "2. 获取一页所有小区的链接\n",
    "3. 获取所有页面的链接\n",
    "4. 获取所有区域的链接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取一个小区页面数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以华龙美晟为例：https://bj.lianjia.com/xiaoqu/1111027375142/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先导入两个库\n",
    "- `requests` 用于获取网页\n",
    "- `from bs4 import BeautifulSoup ` 用于解析数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin  ## 用于 url 拼接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取房价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定位：进入页面后按 `F12` 获取页面信息,然后点击 1 所在的位置，就可以定位到你想要抓取的元素对应的信息。比如想抓取房价，则点击 1 之后再点击房价即可定位到房价所在的标签为 `span` 和 `class=xiaoquUnitPrice` 。用 `find` 或者 `find_all` 根据显示的信息即可定位需要抓取的信息\n",
    "- 抓取：定位完需要抓取信息所在的位置后用 `get_text()` 可以抓取里面的信息，用`[]`取里面标签属性的信息如 `['href']`可以取链接\n",
    "- 存储：一般将抓取的信息存在一个字典里，定义一个字典 `contents`, 然后存入`contents['房价']`中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `find` 找第一个指定的数据\n",
    "- `get_text()` 获取文本内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "房价为: 64154\n"
     ]
    }
   ],
   "source": [
    "href = 'https://bj.lianjia.com/xiaoqu/1111027375142/'\n",
    "### 获取网页数据\n",
    "res = requests.get(href)\n",
    "### 解析网页\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "## 获取房价\n",
    "price = soup.find('span', class_='xiaoquUnitPrice').get_text()\n",
    "print('房价为:', price)\n",
    "\n",
    "## 存入房价数据\n",
    "contents = {}\n",
    "contents['房价'] = price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 小区名称的获取也是类似的思路\n",
    "contents['小区名称'] = soup.find('h1').get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./fig/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小区基本信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./fig/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取小区基本信息思路也是和前面一样：1.定位 2.获取 3.存储\n",
    "\n",
    "从上图获取信息结构可以看出，每条信息对应的标签都一样的，所以只用定位到该标签的上一级标签，用`find_all` 找到包含小区信息的所有的标签，然后遍历该标签即可获得所有小区标签数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "建筑年代 2009年建成 \n",
      "建筑类型 塔楼/塔板结合\n",
      "物业费用 2.38元/平米/月\n",
      "物业公司 北京喜莱达物业管理有限公司\n",
      "开发商 华龙置业房地产开发有限公司\n",
      "楼栋总数 5栋\n",
      "房屋总数 1124户\n",
      "附近门店 新天天家园一店/东城天天家园3号楼1层102\n"
     ]
    }
   ],
   "source": [
    "## 获取小区基本信息的上一级标签信息，并找到包含小区信息的所有标签\n",
    "infos = soup.find('div', class_=\"xiaoquInfo\").find_all('div')\n",
    "## 遍历包含所有小区信息的标签，然后取值和保存\n",
    "for info in infos:\n",
    "    key = info.find('span', class_=\"xiaoquInfoLabel\").get_text()\n",
    "    value = info.find('span', class_=\"xiaoquInfoContent\").get_text()\n",
    "    contents[key] = value\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小区经纬度获取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./fig/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小区经纬度存储在小区信息的最后一个标签里面的 `span`里面，同样原理，定位、获取然后存储\n",
    "\n",
    "因为存储的经纬度放在一起，所以用 `split` 以 `,` 为分隔符分割该字符串分别存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[116.411858,39.869364]\n"
     ]
    }
   ],
   "source": [
    "area = infos[-1].find('span', class_='actshowMap')['xiaoqu']\n",
    "print(area)\n",
    "contents['经度'], contents['维度'] = area[1:-1].split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将获取小区信息所有内容整合成一个函数以便于调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_xiaoqu_content(href):\n",
    "    res = requests.get(href)\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "    contents = {}\n",
    "    \n",
    "    ## 小区名称 \n",
    "    contents['小区名称'] = soup.find('h1').get_text()\n",
    "    \n",
    "    ## 房价\n",
    "    contents['房价'] = soup.find('span', class_=\"xiaoquUnitPrice\").get_text()\n",
    "    \n",
    "    ## 基本信息\n",
    "    infos = soup.find('div', class_=\"xiaoquInfo\").find_all('div')\n",
    "    for info in infos:\n",
    "        key = info.find('span', class_=\"xiaoquInfoLabel\").get_text()\n",
    "        value = info.find('span', class_=\"xiaoquInfoContent\").get_text()\n",
    "        contents[key] = value\n",
    "        \n",
    "    ## 经纬度\n",
    "    contents['经度'], contents['维度'] = info.find('span', class_=\"actshowMap\")['xiaoqu'][1:-1].split(',')\n",
    "    \n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'小区名称': '华龙美晟',\n",
       " '建筑年代': '2009年建成 ',\n",
       " '建筑类型': '塔楼/塔板结合',\n",
       " '开发商': '华龙置业房地产开发有限公司',\n",
       " '房价': '64154',\n",
       " '房屋总数': '1124户',\n",
       " '楼栋总数': '5栋',\n",
       " '物业公司': '北京喜莱达物业管理有限公司',\n",
       " '物业费用': '2.38元/平米/月',\n",
       " '经度': '116.411858',\n",
       " '维度': '39.869364',\n",
       " '附近门店': '新天天家园一店/东城天天家园3号楼1层102'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 调用该函数即可得到该小区的信息\n",
    "get_xiaoqu_content(href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取所有小区的链接\n",
    "有了 `get_xiaoqu_content` 这个函数之后，只要我们能获取所有小区的链接，我们就能得到所有小区的信息，所有我们现在需要做的是获取所有小区的链接。\n",
    "\n",
    "我们能进入的页面是某个区域某一页面的数据如东城区第一页，所有我们想要获得所有小区的链接思路是：\n",
    ">获得所有区域的链接 ==》 获得该区域所有页码链接 ==》 获取每一页面所有小区链接 ==》 调用 `get_xiaoqu_content` 获取所有小区信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取一个区域一页所有小区的链接\n",
    "所以我们先获取一页显示的所有小区的链接，以东城区第一页为例：https://bj.lianjia.com/xiaoqu/dongcheng/\n",
    "\n",
    "从图中个可以到所有小区链接都是在 `<ul class=\"listContent\", log-mod=\"list\">` 的 `li` 里面的 `a` 中的 `href` 属性里面，所有获取的思路是先定位到上一级标签即 `ul`，找到该标签下的所有 `li`， 然后遍历获取即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./fig/4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://bj.lianjia.com/xiaoqu/1111027375142/\n",
      "https://bj.lianjia.com/xiaoqu/1111027380930/\n",
      "https://bj.lianjia.com/xiaoqu/1111027381005/\n",
      "https://bj.lianjia.com/xiaoqu/1111027380567/\n",
      "https://bj.lianjia.com/xiaoqu/1111027382485/\n",
      "https://bj.lianjia.com/xiaoqu/1111027380027/\n",
      "https://bj.lianjia.com/xiaoqu/1111027382283/\n",
      "https://bj.lianjia.com/xiaoqu/1111027375280/\n",
      "https://bj.lianjia.com/xiaoqu/1111027380931/\n",
      "https://bj.lianjia.com/xiaoqu/1111027380242/\n",
      "https://bj.lianjia.com/xiaoqu/1111027374680/\n",
      "https://bj.lianjia.com/xiaoqu/1111027374691/\n",
      "https://bj.lianjia.com/xiaoqu/1111027382190/\n",
      "https://bj.lianjia.com/xiaoqu/1111027374707/\n",
      "https://bj.lianjia.com/xiaoqu/1111027377413/\n",
      "https://bj.lianjia.com/xiaoqu/1111027382035/\n",
      "https://bj.lianjia.com/xiaoqu/1111027379543/\n",
      "https://bj.lianjia.com/xiaoqu/1111027375998/\n",
      "https://bj.lianjia.com/xiaoqu/1111027374300/\n",
      "https://bj.lianjia.com/xiaoqu/1111027375430/\n",
      "https://bj.lianjia.com/xiaoqu/1111027377351/\n",
      "https://bj.lianjia.com/xiaoqu/1111027382286/\n",
      "https://bj.lianjia.com/xiaoqu/1111027381711/\n",
      "https://bj.lianjia.com/xiaoqu/1111027374519/\n",
      "https://bj.lianjia.com/xiaoqu/1111027375206/\n",
      "https://bj.lianjia.com/xiaoqu/1111027377312/\n",
      "https://bj.lianjia.com/xiaoqu/1111027381364/\n",
      "https://bj.lianjia.com/xiaoqu/1111027376748/\n",
      "https://bj.lianjia.com/xiaoqu/1111027380488/\n",
      "https://bj.lianjia.com/xiaoqu/1111027374222/\n"
     ]
    }
   ],
   "source": [
    "url = 'https://bj.lianjia.com/xiaoqu/dongcheng/'\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "lis = soup.find('ul', class_='listContent').find_all('li')\n",
    "for li in lis:\n",
    "    href = li.find('div', class_='title').find('a')['href']\n",
    "    print(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 写成一个函数便于调用, \n",
    "## yield 表示返回生成器，每次调用都返回一个新值，一般用 for 遍历获取所有信息\n",
    "def get_xiaoqu_one_page(url):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "    lis = soup.find('ul', class_='listContent').find_all('li')\n",
    "    for li in lis:\n",
    "        href = li.find('div', class_='title').find('a')['href']\n",
    "        yield href"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取一个区域所有页码链接\n",
    "以东城区第1页为例：https://bj.lianjia.com/xiaoqu/dongcheng/\n",
    "\n",
    "观察第每一页链接，如第2页:`https://bj.lianjia.com/xiaoqu/dongcheng/pg2/` 可以发现不同页面的只是 `pg2` 的数字发生了变化，所以只用得到该区域拥有的所有页面数，通过修改数字，我们就可以构造出所有的页码链接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./fig/5.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://bj.lianjia.com/xiaoqu/dongcheng/'\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 总页码\n",
    "total_page_str = soup.find('div', class_='page-box house-lst-page-box')['page-data']\n",
    "total_page = eval(total_page_str)['totalPage']  ## eval 将字典文本识别为字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg1\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg2\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg3\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg4\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg5\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg6\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg7\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg8\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg9\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg10\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg11\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg12\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg13\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg14\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg15\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg16\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg17\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg18\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg19\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg20\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg21\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg22\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg23\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg24\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg25\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg26\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg27\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg28\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg29\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg30\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg31\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg32\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg33\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg34\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg35\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg36\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg37\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg38\n",
      "https://bj.lianjia.com/xiaoqu/dongcheng/pg39\n"
     ]
    }
   ],
   "source": [
    "## 构造所有页码链接\n",
    "for i in range(1, int(total_page)+1):\n",
    "    href = urljoin(url, 'pg'+ str(i))\n",
    "    print(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 写成函数\n",
    "def get_xiaoqu_all_page(url):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "    ## 总页码\n",
    "    total_page_str = soup.find('div', class_='page-box house-lst-page-box')['page-data']\n",
    "    total_page = eval(total_page_str)['totalPage']  ## eval 将字典文本识别为字典\n",
    "\n",
    "    ## 构造所有页码链接\n",
    "    for i in range(1, int(total_page)+1):\n",
    "        href = urljoin(url, 'pg'+ str(i))\n",
    "        yield href "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取所有区域链接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个和之前获取一页所有链接思路类似，找到区域上一级标签然后遍历,不过我们得到的不是完整的链接，只是部分链接，所有还需要拼接。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./fig/6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://bj.lianjia.com/xiaoqu/dongcheng/\n",
      "https://bj.lianjia.com/xiaoqu/xicheng/\n",
      "https://bj.lianjia.com/xiaoqu/chaoyang/\n",
      "https://bj.lianjia.com/xiaoqu/haidian/\n",
      "https://bj.lianjia.com/xiaoqu/fengtai/\n",
      "https://bj.lianjia.com/xiaoqu/shijingshan/\n",
      "https://bj.lianjia.com/xiaoqu/tongzhou/\n",
      "https://bj.lianjia.com/xiaoqu/changping/\n",
      "https://bj.lianjia.com/xiaoqu/daxing/\n",
      "https://bj.lianjia.com/xiaoqu/yizhuangkaifaqu/\n",
      "https://bj.lianjia.com/xiaoqu/shunyi/\n",
      "https://bj.lianjia.com/xiaoqu/fangshan/\n",
      "https://bj.lianjia.com/xiaoqu/mentougou/\n",
      "https://bj.lianjia.com/xiaoqu/pinggu/\n",
      "https://bj.lianjia.com/xiaoqu/huairou/\n",
      "https://bj.lianjia.com/xiaoqu/miyun/\n",
      "https://bj.lianjia.com/xiaoqu/yanqing/\n",
      "https://lf.lianjia.com/xiaoqu/yanjiao/\n",
      "https://lf.lianjia.com/xiaoqu/xianghe/\n"
     ]
    }
   ],
   "source": [
    "url = 'https://bj.lianjia.com/xiaoqu/'\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "regions = soup.find('div', {'data-role': 'ershoufang'}).find_all('a')\n",
    "base_url = 'https://bj.lianjia.com' ## 用于凭借\n",
    "for region in regions:\n",
    "    href = urljoin(base_url, region['href'])\n",
    "    print(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 写成函数,\n",
    "def get_xiao_regions(url):\n",
    "    url = 'https://bj.lianjia.com/xiaoqu/'\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "    regions = soup.find('div', {'data-role': 'ershoufang'}).find_all('a')\n",
    "    base_url = 'https://bj.lianjia.com' ## 用于凭借\n",
    "    for region in regions:\n",
    "        href = urljoin(base_url, region['href'])\n",
    "        \n",
    "        yield href"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 整合在一起,并增加容错性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin  ## 用于 url 拼接\n",
    "\n",
    "def get_html(url):\n",
    "    headers = {\n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36',\n",
    "            }    \n",
    "    res = requests.get(url, headers = headers)\n",
    "    if res.ok:\n",
    "        soup = BeautifulSoup(res.text, 'lxml')\n",
    "        return soup\n",
    "    else:\n",
    "        print('Error!')\n",
    "\n",
    "    \n",
    "def get_xiaoqu_content(href):\n",
    "    try:\n",
    "        soup = get_html(href)\n",
    "        contents = {}\n",
    "\n",
    "        ## 小区名称 \n",
    "        contents['小区名称'] = soup.find('h1').get_text()\n",
    "\n",
    "        ## 房价\n",
    "        contents['房价'] = soup.find('span', class_=\"xiaoquUnitPrice\").get_text()\n",
    "\n",
    "        ## 基本信息\n",
    "        infos = soup.find('div', class_=\"xiaoquInfo\").find_all('div')\n",
    "        for info in infos:\n",
    "            key = info.find('span', class_=\"xiaoquInfoLabel\").get_text()\n",
    "            value = info.find('span', class_=\"xiaoquInfoContent\").get_text()\n",
    "            contents[key] = value\n",
    "\n",
    "        ## 经纬度\n",
    "        contents['经度'], contents['维度'] = info.find('span', class_=\"actshowMap\")['xiaoqu'][1:-1].split(',')\n",
    "\n",
    "        return contents\n",
    "    except:\n",
    "        print('获取小区数据出错！',href)\n",
    "        \n",
    "def get_xiaoqu_one_page(url):\n",
    "    try:\n",
    "        soup = get_html(url)\n",
    "        lis = soup.find('ul', class_='listContent').find_all('li')\n",
    "        for li in lis:\n",
    "            href = li.find('div', class_='title').find('a')['href']\n",
    "            yield href\n",
    "    except:\n",
    "        print('获取一页小区链接出错！', url)\n",
    "        \n",
    "def get_xiaoqu_all_page(url):\n",
    "    try:\n",
    "        soup = get_html(url)\n",
    "\n",
    "        ## 总页码\n",
    "        total_page_str = soup.find('div', class_='page-box house-lst-page-box')['page-data']\n",
    "        total_page = eval(total_page_str)['totalPage']  ## eval 将字典文本识别为字典\n",
    "\n",
    "        ## 构造所有页码链接\n",
    "        for i in range(1, int(total_page)+1):\n",
    "            href = urljoin(url, 'pg'+ str(i))\n",
    "            \n",
    "            yield href \n",
    "    except:\n",
    "        print('获取所有页面出错!', url)\n",
    "        \n",
    "def get_xiao_regions(url):\n",
    "    try:\n",
    "        soup = get_html(url)\n",
    "        regions = soup.find('div', {'data-role': 'ershoufang'}).find_all('a')\n",
    "        base_url = 'https://bj.lianjia.com' ## 用于拼接\n",
    "        for region in regions:\n",
    "            href = urljoin(base_url, region['href'])\n",
    "\n",
    "            yield href\n",
    "    except:\n",
    "        print('获取所有区域出错!', url)\n",
    "        \n",
    "def get_xiaoqu_all_url(url):\n",
    "    for url_region in get_xiao_regions(url):\n",
    "        for url_page in get_xiaoqu_all_page(url_region):\n",
    "            for url_xiaoqu in get_xiaoqu_one_page(url_page):\n",
    "                yield url_xiaoqu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "url = 'https://bj.lianjia.com/xiaoqu/'\n",
    "\n",
    "for href in get_xiaoqu_all_url:\n",
    "    data.append(get_xiaoqu_content(href))\n",
    "    print('抓取成功：',href)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "540px",
    "left": "0px",
    "right": "1154px",
    "top": "107px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
